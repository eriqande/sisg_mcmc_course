% put all Mike's macros in and begin document
\input{preamble.tex}
\rhead{Session 8 -- Latent variables and Gibbs sampling -- \thepage}




% each \include command puts a new file in.

% to make typesetting faster while working on a single
% section, but still have the appropriate page numbering
% and references, use the \includeonly command



\newlecture{Gibbs Sampling in the inbreeding model\\
and in the simple mixture model}
Goals of this lecture:
\begin{itemize}
\item Introduce the idea of latent variables and show how they lend themselves to Gibbs sampling---a special case of component-wise M-H sampling.
\item Introduce acyclic directed graphs (DAGs) and describe how they can be
useful for identifying the variables involved in a full-conditional distribution.
\end{itemize}



\newslide{Formulating the Model with Latent Variables}
Just think how easy it would be to estimate $f$ and $p$ if we knew whether every individual we sampled was inbred or not:
\enlargethispage*{1000pt}
\begin{center}
\includegraphics[width=.65\textwidth]{illus/InbreedingFlags.pdf}
\end{center}
\begin{itemize}
\item Now, $f$ is just a binomial proportion
\item To estimate $p$ you count both the alleles from Non-inbred individuals, and just one allele from Inbred individuals, and then it too is simply a binomial proportion.
\end{itemize}

\newpage

In other words, if your data look like:
\begin{center}
\includegraphics[width=.9\textwidth]{illus/InbreedingLotsOfFlags.pdf}
\end{center}
\ldots then, to estimate $f$, you don't even need to think about the alleles carried by anyone\ldots

\newpage
\ldots you just have to ``count the flags'':
\begin{center}
\includegraphics[width=\textwidth]{illus/InbreedingFlagsOnly.pdf}
\end{center}



\newpage
To estimate $p$ you just have to count alleles\ldots
\begin{center}
\includegraphics[width=\textwidth]{illus/InbreedingEstimating_p_1.pdf}
\end{center}

\newpage
Keeping in mind each inbred individual contributes just a single gene copy\ldots
\begin{center}
\includegraphics[width=\textwidth]{illus/InbreedingEstimating_p_2.pdf}
\end{center}



\newslide{Joint Density of $p$, $f$, and $\bm{U}$}
\begin{itemize}
\item We denote the $n$ latent variables as $\bm{U}=(U_1,\ldots,U_n)$.
\item $u_i=0$ indicates that $i$ is Not Inbred.  $u_i=1$ indicates $i$ is Inbred.
\item Let $\bm{Y}=(Y_1,\ldots,Y_n)$ denote the genotypes
\end{itemize}
So, the joint density is:
\[
P(\textcolor{Violet}{p},\textcolor{Orange}{f},\textcolor{Green}{\bm{U}},\bm{Y}) = P(\textcolor{Violet}{p})P(\textcolor{Orange}{f})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f}) P(\bm{Y}|\textcolor{Green}{\bm{U}},\textcolor{Violet}{p})
\]
and, so, the posterior is:
\[
P(\textcolor{Violet}{p},\textcolor{Orange}{f},\textcolor{Green}{\bm{U}}|\bm{Y}) = \frac{P(\textcolor{Violet}{p},\textcolor{Orange}{f},\textcolor{Green}{\bm{U}},\bm{Y})}
{ \int_{\textcolor{Violet}{p}} \int_{\textcolor{Orange}{f}} \sum_{0\leq  \textcolor{Green}{U_1}\leq 1}
\cdots
\sum_{0\leq  \textcolor{Green}{U_n}\leq 1}
P(\textcolor{Violet}{p},\textcolor{Orange}{f},\textcolor{Green}{\bm{U}},\bm{Y})d\textcolor{Violet}{p}d\textcolor{Orange}{f}
}
\]
The normalizing constant is even nastier!  It seems we have gained little, until we remember that we don't have to mess with the normalizing constant, \ldots but even then it's not immediately clear we have gained anything.

\newslide{Generic MCMC for $f$ and $p$ with latent variables}
\begin{itemize}
\item Recall, we wish to simulate $f$ and $p$ from their posterior distributions and so learn about $P(f,p|\bm{Y})$.
\item However, now, our chain is moving about the space of $f$, $p$, {\em and} $\bm{U}$\ldots
\begin{itemize}
\item it visits a sequence of states of the form $(f^{(1)},p^{(1)},\bm{U}^{(1)})$, $(f^{(2)},p^{(2)},\bm{U}^{(2)})$, $(f^{(3)},p^{(3)},\bm{U}^{(3)}), \ldots$
\end{itemize}
\mbox{}\vspace*{.25in}
\item {\bf Important Concept}: Obtaining the Marginal Posterior Distribution from MCMC output is simple:
\begin{itemize}
\item From the sequence $(f^{(1)},p^{(1)},\bm{U}^{(1)})$, $(f^{(2)},p^{(2)},\bm{U}^{(2)}),\ldots$ you can just focus on collecting the values $(f^{(1)},p^{(1)})$, $(f^{(2)}),\ldots$, discarding the $\bm{U}^{(\cdot)}$'s if you are not interested in their posterior distribution.  (Note, though, that you might be interested in the individual $U_i$'s!).
\end{itemize}
\end{itemize}



\newslide{Naive Metropolis-Hastings for $f$, $p$, and $\bm{U}$}
\begin{enumerate}
\item Do an update for \textcolor{Violet}{$p$}:
\begin{enumerate}
\item propose $\textcolor{Violet}{p^*}$ from $\mathrm{Normal}(p,s_p)$
\item accept or reject on the basis of 
\[
\frac{\textcolor{Violet}{q(p|p^*)}}{\textcolor{Violet}{q(p^*|p)} } \times
\frac{ P(\textcolor{Violet}{p^*})P(\textcolor{Orange}{f})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f}) P(\bm{Y}|\textcolor{Green}{\bm{U}},\textcolor{Violet}{p^*})}
{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f}) P(\bm{Y}|\textcolor{Green}{\bm{U}},\textcolor{Violet}{p})}
\]
\end{enumerate}


\item Do an update for \textcolor{Orange}{$f$}:
\begin{enumerate}
\item propose $\textcolor{Orange}{f^*}$ from $\mathrm{Normal}(f,s_f)$
\item accept or reject on the basis of 
\[
\frac{\textcolor{Orange}{q(f|f^*)}}{\textcolor{Orange}{q(f^*|f)} } \times
\frac{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f^*})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f^*}) P(\bm{Y}|\textcolor{Green}{\bm{U}},\textcolor{Violet}{p})}
{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f}) P(\bm{Y}|\textcolor{Green}{\bm{U}},\textcolor{Violet}{p})}
\]
\end{enumerate}
\newpage
\item Do an update for \textcolor{Green}{$\bm{U}$} ($n$ separate updates, one for each \textcolor{Green}{$U_i$}).


For $i$ in $1,\ldots,n$:
\begin{enumerate}
\item Propose a new \textcolor{Green}{$U^*_i$} by flipping a coin (heads=0, tails=1).
\item Accept or reject on the basis of 
\[
\frac{\textcolor{Green}{q(U_i|U_i^*)}}{\textcolor{Green}{q(U_i^*|U_i)} } \times
\frac{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f})P(\textcolor{Green}{\bm{U}^*}|\textcolor{Orange}{f}) P(\bm{Y}|\textcolor{Green}{\bm{U}^*},\textcolor{Violet}{p})}
{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f}) P(\bm{Y}|\textcolor{Green}{\bm{U}},\textcolor{Violet}{p})}
\] 
\[
=
\frac{\textcolor{Green}{q(U_i|U_i^*)}}{\textcolor{Green}{q(U_i^*|U_i)} } \times
\frac{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f})P(\textcolor{Green}{U_i^*}|\textcolor{Orange}{f}) P(\bm{Y}|\textcolor{Green}{U_i^*},\textcolor{Violet}{p})}
{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f})P(\textcolor{Green}{U_i}|\textcolor{Orange}{f}) P(\bm{Y}|\textcolor{Green}{U_i},\textcolor{Violet}{p})}
\] 
\end{enumerate}
\end{enumerate}

Note the cancellations in all these Hastings Ratios.

\fbox{Computer Demo: {\tt inbred\_p} {\small (``Silly'' using (S), (f), and (p))}}~\footnote{which will segue into Gibbs sampling}

\newslide{Gibbs Sampling}
Gibbs sampling is a special case of the component-wise M-H sampler in which the proposal distribution is the {\em full conditional distribution}.

Gibbs Sampling Step for \textcolor{Orange}{$f$}:
\begin{itemize}
\item Recall the generic component-wise M-H update:
\[
\frac{\textcolor{Orange}{q(f|f^*)}}{\textcolor{Orange}{q(f^*|f)} } \times
\frac{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f^*})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f^*}) P(\bm{Y}|\textcolor{Green}{\bm{U}},\textcolor{Violet}{p})}
{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f}) P(\bm{Y}|\textcolor{Green}{\bm{U}},\textcolor{Violet}{p})}
\]


The full conditional distribution for \textcolor{Orange}{$f$} is the distribution of \textcolor{Orange}{$f$} conditional upon the current values of all other variables in the model.  This must be proportional to the product of all the factors in the joint density that have \textcolor{Orange}{$f$} in them: $P(\textcolor{Orange}{f})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f})$

\newpage 
\item\begin{minipage}{.4\textwidth}
The $P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f})$ portion of the joint density pertains to ``counting up our flags''
\end{minipage}
~~~~
\begin{minipage}{.5\textwidth}
\includegraphics[width=.84\textwidth]{illus/InbreedingFlagsOnly.pdf}
\end{minipage}

\item $P(\textcolor{Orange}{f})$ is the prior for $\textcolor{Orange}{f}$

\item So the full conditional for $\textcolor{Orange}{f}$ is a beta distribution---it's the ``posterior'' for $\textcolor{Orange}{f}$ given the ``data'' \textcolor{Green}{$\bm{U}$}

\item Letting the proposal $q(\cdot|\cdot)$ be that full conditional gives us a Hastings Ratio of 
\[
\frac{P(\textcolor{Orange}{f})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f})/C}
{P(\textcolor{Orange}{f^*})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f^*})/C}
\times
\frac{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f^*})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f^*}) P(\bm{Y}|\textcolor{Green}{\bm{U}},\textcolor{Violet}{p})}
{ P(\textcolor{Violet}{p})P(\textcolor{Orange}{f})P(\textcolor{Green}{\bm{U}}|\textcolor{Orange}{f}) P(\bm{Y}|\textcolor{Green}{\bm{U}},\textcolor{Violet}{p})}
\]
\item Which is 1---This is always the case with Gibbs sampling---you always accept the proposal from the full conditional distribution.
\end{itemize}




\newslide{A Directed Graphical View of The Model\ldots}
\enlargethispage*{1000pt}
\begin{center}
\includegraphics[width=.6\textwidth]{illus/InbreedingDAG.pdf}
\end{center}
\ldots makes it fairly easy to envision how Gibbs updates for \textcolor{Violet}{$p$} and \textcolor{Green}{$\bm{U}$} would proceed.  \hfill\fbox{Computer Demo: {\tt inbred\_p} {\small (``Gibbs'' using (g))}}







\newslide{DAG notation and terminology}
\begin{center}
\includegraphics[width=\textwidth]{illus/dagnotat.pdf}
\end{center}





\newslide{A reminder that all inference is conditional upon the underlying model\ldots and a small step toward {\em structure}}
\vspace*{-.25in}
\enlargethispage*{1000pt}
\begin{minipage}{.43\textwidth}
We could have attributed the departure from Hardy-Weinberg proportions to a mixture (in the proportions of $\pi=.5$) of two populations with allele frequencies $p_1$ and $p2$, respectively.
\end{minipage}
\hfill
\begin{minipage}{.5\textwidth}
\includegraphics[width=\textwidth]{illus/OneLocusDAG.pdf}
\end{minipage}

This is, in fact, the {\em structure} model with no admixture, $K=2$, and a single locus.

Gibbs sampling is straightforward\ldots\hfill\fbox{{\footnotesize Computer Demo:} {\tiny {\tt newhybs}  via {\tt RunNewHybs.sh}}}





\newslide{The simple genetic mixture model}

Gotta throw some more slides in here.




\newslide{Wrap-Up}
Main Points:
\begin{itemize}
\item In problems where it is useful, MCMC almost always proceeds by proposing changes to a small subset of the variables.
\item There may be many different proposal types.
\begin{itemize}
\item Each proposal type {\em must} satisfy detailed balance.
\item Each proposal type need not make an irreducible chain, BUT
\item All proposals taken together should form an irreducible chain.
\end{itemize}
\item Latent variables may help in factorizing complex joint densities and lend themselves to Gibbs sampling.
\item Simple proposals may be fast to implement, but may not lead to good mixing of the chain.
\item Designing good proposal distributions is where one gets to perform ``art" in implementing MCMC.
\end{itemize}

\end{document}

